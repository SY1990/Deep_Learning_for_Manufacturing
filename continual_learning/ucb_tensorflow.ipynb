{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_probability\\python\\layers\\util.py:104: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From c:\\users\\sinha_s\\appdata\\local\\continuum\\anaconda3\\envs\\tf2_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\linalg\\linear_operator_diag.py:166: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_flipout (Conv3DFlipou (None, 30, 30, 30, 32)    24032     \n",
      "_________________________________________________________________\n",
      "conv3d_flipout_1 (Conv3DFlip (None, 14, 14, 14, 32)    131104    \n",
      "_________________________________________________________________\n",
      "conv3d_flipout_2 (Conv3DFlip (None, 12, 12, 12, 32)    55328     \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 6, 6, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6912)              0         \n",
      "_________________________________________________________________\n",
      "dense_flipout (DenseFlipout) (None, 128)               1769600   \n",
      "_________________________________________________________________\n",
      "dense_flipout_1 (DenseFlipou (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_flipout_2 (DenseFlipou (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "distribution_lambda (Distrib ((None, 6), (None, 6))    0         \n",
      "=================================================================\n",
      "Total params: 1,997,286\n",
      "Trainable params: 1,997,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define Bayes Model\n",
    "def bayes_model_arch(voxel_dim=64,deviation_channels=3,output_dimension=6):\n",
    "    kl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) / tf.cast(4000, dtype=tf.float32))\n",
    "    negloglik = lambda y, rv_y: -rv_y.log_prob(y)\n",
    "    aleatoric_std=0.001\n",
    "    aleatoric_tensor=[aleatoric_std] * output_dimension\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(voxel_dim,voxel_dim,voxel_dim,deviation_channels)),\n",
    "    tfp.layers.Convolution3DFlipout(32, kernel_size=(5,5,5),kernel_divergence_fn=kl_divergence_function,strides=(2,2,2),activation=tf.nn.relu),\n",
    "    tfp.layers.Convolution3DFlipout(32, kernel_size=(4,4,4),kernel_divergence_fn=kl_divergence_function,strides=(2,2,2),activation=tf.nn.relu),\n",
    "    tfp.layers.Convolution3DFlipout(32, kernel_size=(3,3,3),kernel_divergence_fn=kl_divergence_function,strides=(1,1,1),activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling3D(pool_size=[2, 2, 2]),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tfp.layers.DenseFlipout(128,activation=tf.nn.relu,kernel_divergence_fn=kl_divergence_function),\n",
    "    tfp.layers.DenseFlipout(64,kernel_divergence_fn=kl_divergence_function,activation=tf.nn.relu),\n",
    "    tfp.layers.DenseFlipout(output_dimension,kernel_divergence_fn=kl_divergence_function),\n",
    "    tfp.layers.DistributionLambda(lambda t:tfd.MultivariateNormalDiag(loc=t[..., :output_dimension], scale_diag=aleatoric_tensor)),])\n",
    "\n",
    "    #negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
    "    #experimental_run_tf_function=False\n",
    "    #tf.keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),experimental_run_tf_function=False,loss=negloglik,metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "    #print(\"3D CNN model successfully compiled\")\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "model=bayes_model_arch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1e793e68048>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading Model Weights\n",
    "part_type=\"inner_rf_assembly\"\n",
    "train_path='../trained_models/'+part_type\n",
    "weight_path=train_path+'/model'+'/Bayes_trained_model_0'\n",
    "model.load_weights(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv3d_flipout (5, 5, 5, 3, 32)\n",
      "conv3d_flipout_1 (4, 4, 4, 32, 32)\n",
      "conv3d_flipout_2 (3, 3, 3, 32, 32)\n",
      "No weights for the following layer:  max_pooling3d\n",
      "No weights for the following layer:  flatten\n",
      "dense_flipout (6912, 128)\n",
      "dense_flipout_1 (128, 64)\n",
      "dense_flipout_2 (64, 6)\n",
      "No weights for the following layer:  distribution_lambda\n",
      "6\n",
      "Computing Weight Importance...\n",
      "(5, 5, 5, 3, 32)\n",
      "(4, 4, 4, 32, 32)\n",
      "(3, 3, 3, 32, 32)\n",
      "(6912, 128)\n",
      "(128, 64)\n",
      "(64, 6)\n"
     ]
    }
   ],
   "source": [
    "# Template for extracting weights [0] mean weights , [1] Std Dev Weights ,[2] Bias (deterministic)\n",
    "weight_mean_val=[]\n",
    "weight_std_val=[]\n",
    "\n",
    "#Get Weights for each layer\n",
    "for layer in model.layers:\n",
    "    try:\n",
    "        weight_mean_val.append(layer.get_weights()[0])\n",
    "        #Weight Std to be mapped to importance (importnace incersely proporational to learning rate)\n",
    "        weight_std_val.append(layer.get_weights()[1])\n",
    "        print(layer.name,layer.get_weights()[0].shape)\n",
    "    except:\n",
    "        print(\"No weights for the following layer: \",layer.name)\n",
    "\n",
    "print(len(weight_std_val))\n",
    "\n",
    "print(\"Computing Weight Importance...\")\n",
    "\n",
    "weight_importance=[]\n",
    "base_lr=0.001\n",
    "for weight_std in weight_std_val:\n",
    "    weight_importance.append(base_lr*np.reciprocal(weight_std))\n",
    "    print(np.reciprocal(weight_std).shape)\n",
    "\n",
    "param_wise_lr=weight_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Data Using data import module\n",
    "len(model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'sequential_3/dense_7/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[ 0.6258982 , -0.4979097 ,  0.28686547, -0.01360267,  0.05580539,\n",
       "         -0.56882447, -0.6072071 , -0.55717945,  0.5352369 ,  0.11377299],\n",
       "        [-0.16711238,  0.30352652,  0.14272827,  0.20083952,  0.51501626,\n",
       "          0.15367812,  0.60684675, -0.42796087, -0.6226447 , -0.39787495],\n",
       "        [ 0.41024953, -0.53364325,  0.00331223, -0.07450342,  0.03721464,\n",
       "          0.4702887 ,  0.433626  ,  0.369016  ,  0.03784174, -0.07247078],\n",
       "        [ 0.37795883, -0.2091943 , -0.19328001, -0.20671052, -0.604131  ,\n",
       "          0.5799908 ,  0.29559696,  0.01463783, -0.62099135, -0.49084884],\n",
       "        [-0.01541686,  0.6160254 ,  0.6006045 ,  0.5407329 ,  0.4209569 ,\n",
       "         -0.44127223,  0.25320876,  0.5104522 , -0.28755203,  0.6040004 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'sequential_3/dense_7/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'sequential_3/dense_8/kernel:0' shape=(10, 10) dtype=float32, numpy=\n",
       " array([[-0.13439831, -0.504954  ,  0.13725948,  0.02526957, -0.4135151 ,\n",
       "          0.01591569, -0.08402494, -0.50864893, -0.20031807,  0.43479466],\n",
       "        [-0.35914874,  0.16361469, -0.07119748,  0.2240259 , -0.29967225,\n",
       "          0.12107527,  0.24101388,  0.38838202, -0.32806936, -0.22133768],\n",
       "        [ 0.14802665, -0.33450067,  0.180911  ,  0.37133354,  0.5430635 ,\n",
       "          0.46353662, -0.3727994 , -0.47709277, -0.01575691, -0.48423406],\n",
       "        [ 0.01842535, -0.28569183,  0.14190394, -0.28565437, -0.11657628,\n",
       "          0.22777146,  0.10609168, -0.4142004 , -0.48440605, -0.1098983 ],\n",
       "        [ 0.42744845, -0.01057023,  0.15890783,  0.34113133, -0.35845125,\n",
       "          0.19429761, -0.21103302,  0.26148552,  0.06578225,  0.20395243],\n",
       "        [ 0.11385012,  0.1732676 ,  0.49303544,  0.32476473, -0.2845718 ,\n",
       "         -0.39444476,  0.36203277, -0.54473776, -0.03730696, -0.15816465],\n",
       "        [-0.49634722, -0.12584445, -0.09262618,  0.48710084, -0.05905846,\n",
       "         -0.30271375, -0.12841833,  0.10472733,  0.25183594,  0.23533463],\n",
       "        [ 0.1719988 , -0.5346222 ,  0.310508  ,  0.544693  ,  0.54609275,\n",
       "         -0.20907801, -0.41839683, -0.31905648,  0.53110325, -0.12677982],\n",
       "        [ 0.09309226,  0.15442294,  0.42697626, -0.5131897 ,  0.48096764,\n",
       "         -0.28113487,  0.11475903,  0.02500826,  0.0752551 , -0.53700227],\n",
       "        [-0.4269978 ,  0.19458455,  0.4089133 ,  0.3423406 ,  0.24539965,\n",
       "          0.33209026,  0.225488  , -0.54145724,  0.16492033,  0.34705007]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'sequential_3/dense_8/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Otaining Gradient Structure for model\n",
    "x = tf.random.normal([7, 5])\n",
    "test_model = tf.keras.Sequential([tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "                                  tf.keras.layers.Flatten(),\n",
    "                             tf.keras.layers.Dense(10, activation=tf.nn.relu)])\n",
    "#layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)\n",
    "\n",
    "with tf.GradientTape() as t2:\n",
    "  # The inner tape only takes the gradient with respect to the input,\n",
    "  # not the variables.\n",
    "  with tf.GradientTape(watch_accessed_variables=False) as t1:\n",
    "    t1.watch(x)\n",
    "    y = test_model(x)\n",
    "    out = tf.reduce_sum(test_model(x)**2)\n",
    "  # 1. Calculate the input gradient.\n",
    "  g1 = t1.gradient(out, x)\n",
    "  # 2. Calculate the magnitude of the input gradient.\n",
    "  g1_mag = tf.norm(g1)\n",
    "\n",
    "# 3. Calculate the gradient of the magnitude with respect to the model.\n",
    "dg1_mag = t2.gradient(g1_mag, test_model.trainable_variables)\n",
    "test_model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dummy Gradient Application\n",
    "g1_mag\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "optimizer.apply_gradients(zip(dg1_mag*2,layer.trainable_variables))\n",
    "#Key is change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10)\n",
      "(10,)\n",
      "(10, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# Multiply Gradients with importance factor before application\n",
    "importance=test_model.trainable_variables\n",
    "\n",
    "def update_gradients(gradients, importance):\n",
    "    grad_importance=[]\n",
    "    for grad_array,importance_array in zip(gradients,importance):\n",
    "        grad_importance_array=grad_array*importance_array\n",
    "        print(grad_importance_array.shape)\n",
    "        grad_importance.append(grad_importance_array)\n",
    "    \n",
    "    return grad_importance\n",
    "\n",
    "grad_importance=update_gradients(dg1_mag,importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_2/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[ 0.40520763,  0.01090676,  0.56578726, -0.3407245 ,  0.02358539,\n",
       "         -0.40111548,  0.13548976, -0.5364721 , -0.03973733, -0.2508733 ],\n",
       "        [-0.3281536 , -0.22694927, -0.55398303,  0.3664626 , -0.6103132 ,\n",
       "          0.15862253,  0.068159  , -0.03223365, -0.57953167,  0.22649029],\n",
       "        [ 0.4044137 , -0.01696028,  0.15214351,  0.2334536 , -0.5133557 ,\n",
       "          0.21818036, -0.01759515, -0.5542543 ,  0.5166201 , -0.00869151],\n",
       "        [ 0.1070876 ,  0.32423657,  0.11652444, -0.54368824, -0.28556326,\n",
       "          0.4796999 , -0.53643286, -0.57631475, -0.3853297 ,  0.41003042],\n",
       "        [-0.26340643,  0.5624035 , -0.43809342, -0.08927061,  0.3437686 ,\n",
       "          0.13532296, -0.25898913, -0.44747776, -0.13408372, -0.03619962]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(10,) dtype=float32, numpy=\n",
       " array([-0.03283736, -0.00368406, -0.03254008, -0.01275273, -0.00168085,\n",
       "        -0.02191626, -0.00463928, -0.0167179 , -0.0358901 , -0.0149522 ],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Train Step\n",
    "def step(real_x, real_y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Make prediction\n",
    "        pred_y = model(real_x.reshape((-1, 64, 64, 64,3)))\n",
    "        # Calculate loss\n",
    "        model_loss = tf.keras.losses.mse(real_y, pred_y)\n",
    "    \n",
    "    # Calculate gradients\n",
    "    model_gradients = tape.gradient(model_loss, model.trainable_variables)\n",
    "    \n",
    "    # Update model\n",
    "    continual_learning(model.trainable_variables,model_gradients,param_wise_lr)\n",
    "    #optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Learning Rate Updates for Continual Learning\n",
    "\n",
    "def continual_learning(model_variables,model_gradients,param_wise_lr):\n",
    "    #Iterate through all variables to optimize at diffrent rates \n",
    "    new_grad_vars=[]\n",
    "    for grad,var in model_gradients:\n",
    "        grad*=param_wise_lr\n",
    "        new_grad_vars.append((grad,var))\n",
    "        \n",
    "    optimizer.apply_gradients(new_grad_vars)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "bat_per_epoch = math.floor(len(x_train) / batch_size)\n",
    "for epoch in range(epochs):\n",
    "    print('=', end='')\n",
    "    for i in range(bat_per_epoch):\n",
    "        n = i*batch_size\n",
    "        step(x_train[n:n+batch_size], y_train[n:n+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile for evluation if required\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.categorical_crossentropy, metrics=['acc']) # Compile just for evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
