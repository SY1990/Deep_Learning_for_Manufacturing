{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Benchmarking for Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # Nvidia Quadro GV100\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # Nvidia Quadro M2000\n",
    "\n",
    "#Adding Path to various Modules\n",
    "sys.path.append(\"../core\")\n",
    "sys.path.append(\"../visualization\")\n",
    "sys.path.append(\"../utilities\")\n",
    "sys.path.append(\"../datasets\")\n",
    "sys.path.append(\"../trained_models\")\n",
    "sys.path.append(\"../config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing Required Modules\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "#Importing Config files\n",
    "import assembly_config as config\n",
    "import model_config as cftrain\n",
    "import hybrid_utils as hy_util\n",
    "#Importing required modules from the package\n",
    "from measurement_system import HexagonWlsScanner\n",
    "from assembly_system import VRMSimulationModel\n",
    "from wls400a_system import GetInferenceData\n",
    "from data_import import GetTrainData\n",
    "from encode_decode_model import Encode_Decode_Model\n",
    "from training_viz import TrainViz\n",
    "from metrics_eval import MetricsEval\n",
    "from keras_lr_multiplier import LRMultiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing from Assembly Config File....\n"
     ]
    }
   ],
   "source": [
    "print('Parsing from Assembly Config File....')\n",
    "\n",
    "data_type=config.assembly_system['data_type']\n",
    "application=config.assembly_system['application']\n",
    "part_type=config.assembly_system['part_type']\n",
    "part_name=config.assembly_system['part_name']\n",
    "data_format=config.assembly_system['data_format']\n",
    "assembly_type=config.assembly_system['assembly_type']\n",
    "assembly_kccs=config.assembly_system['assembly_kccs']\n",
    "assembly_kpis=config.assembly_system['assembly_kpis']\n",
    "voxel_dim=config.assembly_system['voxel_dim']\n",
    "point_dim=config.assembly_system['point_dim']\n",
    "voxel_channels=config.assembly_system['voxel_channels']\n",
    "noise_type=config.assembly_system['noise_type']\n",
    "mapping_index=config.assembly_system['mapping_index']\n",
    "\n",
    "system_noise=config.assembly_system['system_noise']\n",
    "aritifical_noise=config.assembly_system['aritifical_noise']\n",
    "data_folder=config.assembly_system['data_folder']\n",
    "kcc_folder=config.assembly_system['kcc_folder']\n",
    "kcc_files=config.assembly_system['kcc_files']\n",
    "test_kcc_files=config.assembly_system['test_kcc_files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing from Training Config File\n"
     ]
    }
   ],
   "source": [
    "#added for hybrid model\n",
    "categorical_kccs=config.assembly_system['categorical_kccs']\n",
    "\n",
    "print('Parsing from Training Config File')\n",
    "\n",
    "model_type=cftrain.model_parameters['model_type']\n",
    "output_type=cftrain.model_parameters['output_type']\n",
    "batch_size=cftrain.model_parameters['batch_size']\n",
    "epocs=cftrain.model_parameters['epocs']\n",
    "split_ratio=cftrain.model_parameters['split_ratio']\n",
    "optimizer=cftrain.model_parameters['optimizer']\n",
    "loss_func=cftrain.model_parameters['loss_func']\n",
    "regularizer_coeff=cftrain.model_parameters['regularizer_coeff']\n",
    "activate_tensorboard=cftrain.model_parameters['activate_tensorboard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file Structure....\n"
     ]
    }
   ],
   "source": [
    "print('Creating file Structure....')\n",
    "\n",
    "bn_model_name='fcnn_bn'\n",
    "\n",
    "folder_name=part_type\n",
    "train_path='../trained_models/'+part_type\n",
    "pathlib.Path(train_path).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "train_path=train_path+'/'+bn_model_name\n",
    "pathlib.Path(train_path).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "model_path=train_path+'/models'\n",
    "pathlib.Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logs_path=train_path+'/logs'\n",
    "pathlib.Path(logs_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plots_path=train_path+'/plots'\n",
    "pathlib.Path(plots_path).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the Assembly System and Measurement System....\n",
      "Valid Output Stages and heads\n",
      "KCC sub-listing:  0\n",
      "Process Parameter Dimension:  157\n"
     ]
    }
   ],
   "source": [
    "#Objects of Measurement System, Assembly System, Get Inference Data\n",
    "print('Initializing the Assembly System and Measurement System....')\n",
    "measurement_system=HexagonWlsScanner(data_type,application,system_noise,part_type,data_format)\n",
    "vrm_system=VRMSimulationModel(assembly_type,assembly_kccs,assembly_kpis,part_name,part_type,voxel_dim,voxel_channels,point_dim,aritifical_noise)\n",
    "get_data=GetTrainData()\n",
    "\n",
    "\n",
    "kcc_sublist=cftrain.encode_decode_params['kcc_sublist']\n",
    "output_heads=cftrain.encode_decode_params['output_heads']\n",
    "encode_decode_multi_output_construct=config.encode_decode_multi_output_construct\n",
    "\n",
    "if(output_heads==len(encode_decode_multi_output_construct)):\n",
    "\tprint(\"Valid Output Stages and heads\")\n",
    "else:\n",
    "\tprint(\"Inconsistent model setting\")\n",
    "\n",
    "print(\"KCC sub-listing: \",kcc_sublist)\n",
    "\n",
    "#Check for KCC sub-listing\n",
    "if(kcc_sublist!=0):\n",
    "\toutput_dimension=len(kcc_sublist)\n",
    "else:\n",
    "\toutput_dimension=assembly_kccs\n",
    "\n",
    "print(\"Process Parameter Dimension: \",output_dimension)\n",
    "\n",
    "input_size=(voxel_dim,voxel_dim,voxel_dim,voxel_channels)\n",
    "\n",
    "model_depth=cftrain.encode_decode_params['model_depth']\n",
    "inital_filter_dim=cftrain.encode_decode_params['inital_filter_dim']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all Process Parameter\n"
     ]
    }
   ],
   "source": [
    "#importing file names for model input\n",
    "input_file_names_x=config.encode_decode_construct['input_data_files_x']\n",
    "input_file_names_y=config.encode_decode_construct['input_data_files_y']\n",
    "input_file_names_z=config.encode_decode_construct['input_data_files_z']\n",
    "\n",
    "input_dataset=[]\n",
    "input_dataset.append(get_data.data_import(input_file_names_x,data_folder))\n",
    "input_dataset.append(get_data.data_import(input_file_names_y,data_folder))\n",
    "input_dataset.append(get_data.data_import(input_file_names_z,data_folder))\n",
    "\n",
    "kcc_dataset=get_data.data_import(kcc_files,kcc_folder)\n",
    "\n",
    "if(kcc_sublist!=0):\n",
    "\tprint(\"Sub-setting Process Parameters: \",kcc_sublist)\n",
    "\tkcc_dataset=kcc_dataset.iloc[:,kcc_sublist]\n",
    "\ttest_kcc_dataset=test_kcc_dataset[:,kcc_sublist]\n",
    "else:\n",
    "\tprint(\"Using all Process Parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:42<00:00, 11.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not convergent solutions:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing to point cloud data\n",
    "point_index=get_data.load_mapping_index(mapping_index)\n",
    "input_conv_data, kcc_subset_dump,kpi_subset_dump=get_data.data_convert_voxel_mc(vrm_system,input_dataset,point_index,kcc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing output data for stage:  {'stage_id': 2, 'output_data_files_x': ['DX_crossmember_test1_3.csv'], 'output_data_files_y': ['DY_crossmember_test1_3.csv'], 'output_data_files_z': ['DZ_crossmember_test1_3.csv'], 'output_test_data_files_x': ['DX_crossmember_test1_3.csv'], 'output_test_data_files_y': ['DY_crossmember_test1_3.csv'], 'output_test_data_files_z': ['DZ_crossmember_test1_3.csv']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:42<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not convergent solutions:  0\n",
      "Importing output data for stage:  {'stage_id': 6, 'output_data_files_x': ['DX_crossmember_test1_7.csv'], 'output_data_files_y': ['DY_crossmember_test1_7.csv'], 'output_data_files_z': ['DZ_crossmember_test1_7.csv'], 'output_test_data_files_x': ['DX_crossmember_test1_7.csv'], 'output_test_data_files_y': ['DY_crossmember_test1_7.csv'], 'output_test_data_files_z': ['DZ_crossmember_test1_7.csv']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:42<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not convergent solutions:  0\n"
     ]
    }
   ],
   "source": [
    "y_shape_error_list=[]\n",
    "\n",
    "for encode_decode_construct in encode_decode_multi_output_construct:\n",
    "\t#importing file names for model output\n",
    "\tprint(\"Importing output data for stage: \",encode_decode_construct)\n",
    "\n",
    "\toutput_file_names_x=encode_decode_construct['output_data_files_x']\n",
    "\toutput_file_names_y=encode_decode_construct['output_data_files_y']\n",
    "\toutput_file_names_z=encode_decode_construct['output_data_files_z']\n",
    "\n",
    "\toutput_dataset=[]\n",
    "\toutput_dataset.append(get_data.data_import(output_file_names_x,data_folder))\n",
    "\toutput_dataset.append(get_data.data_import(output_file_names_y,data_folder))\n",
    "\toutput_dataset.append(get_data.data_import(output_file_names_z,data_folder))\n",
    "\n",
    "\toutput_conv_data, kcc_subset_dump,kpi_subset_dump=get_data.data_convert_voxel_mc(vrm_system,output_dataset,point_index,kcc_dataset)\n",
    "\n",
    "\ty_shape_error_list.append(output_conv_data)\n",
    "\n",
    "shape_error=np.concatenate(y_shape_error_list, axis=4)\n",
    "kcc_regression,kcc_classification=hy_util.split_kcc(kcc_subset_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Benchmarking Input Output\n",
    "# 1 Only Regression\n",
    "# 2 Only Classfication\n",
    "# 3 Only Shape Error\n",
    "# 4 Regression + Classification\n",
    "# 5 Regression + OSE\n",
    "# 6 Regression + Classification\n",
    "# 7 Regression + Classification + OSE\n",
    "#Select Option to execute\n",
    "option_num=3\n",
    "\n",
    "from bm_selector import get_bm_io\n",
    "\n",
    "y_out_list=get_bm_io(option_num,kcc_regression,kcc_classification,shape_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model using Keras Tuner\n",
    "\n",
    "#Get Base Model\n",
    "#Reloading to change cache\n",
    "import importlib\n",
    "import dl_models\n",
    "importlib.reload(dl_models)\n",
    "\n",
    "dl_model_obj2=dl_models.DL_BM_Arch(output_dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D FCNN model successfully compiled\n",
      "Model: \"FCNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 64, 64, 64, 3)]   0         \n",
      "_________________________________________________________________\n",
      "Conv0_1 (Conv3D)             (None, 64, 64, 64, 32)    2624      \n",
      "_________________________________________________________________\n",
      "Act0_1 (Activation)          (None, 64, 64, 64, 32)    0         \n",
      "_________________________________________________________________\n",
      "MaxPooling0_1 (MaxPooling3D) (None, 32, 32, 32, 32)    0         \n",
      "_________________________________________________________________\n",
      "Conv1_1 (Conv3D)             (None, 32, 32, 32, 64)    55360     \n",
      "_________________________________________________________________\n",
      "Act1_1 (Activation)          (None, 32, 32, 32, 64)    0         \n",
      "_________________________________________________________________\n",
      "MaxPooling1_1 (MaxPooling3D) (None, 16, 16, 16, 64)    0         \n",
      "_________________________________________________________________\n",
      "Conv2_1 (Conv3D)             (None, 16, 16, 16, 128)   221312    \n",
      "_________________________________________________________________\n",
      "Act2_1 (Activation)          (None, 16, 16, 16, 128)   0         \n",
      "_________________________________________________________________\n",
      "MaxPooling2_1 (MaxPooling3D) (None, 8, 8, 8, 128)      0         \n",
      "_________________________________________________________________\n",
      "UpSampling2_1 (UpSampling3D) (None, 16, 16, 16, 128)   0         \n",
      "_________________________________________________________________\n",
      "upConvSam2_1 (Conv3D)        (None, 16, 16, 16, 128)   131200    \n",
      "_________________________________________________________________\n",
      "upConv2_1 (Conv3D)           (None, 16, 16, 16, 128)   442496    \n",
      "_________________________________________________________________\n",
      "upAct2_1 (Activation)        (None, 16, 16, 16, 128)   0         \n",
      "_________________________________________________________________\n",
      "upAct2_2 (Activation)        (None, 16, 16, 16, 128)   0         \n",
      "_________________________________________________________________\n",
      "UpSampling1_1 (UpSampling3D) (None, 32, 32, 32, 128)   0         \n",
      "_________________________________________________________________\n",
      "upConvSam1_1 (Conv3D)        (None, 32, 32, 32, 64)    65600     \n",
      "_________________________________________________________________\n",
      "upConv1_1 (Conv3D)           (None, 32, 32, 32, 64)    110656    \n",
      "_________________________________________________________________\n",
      "upAct1_1 (Activation)        (None, 32, 32, 32, 64)    0         \n",
      "_________________________________________________________________\n",
      "upAct1_2 (Activation)        (None, 32, 32, 32, 64)    0         \n",
      "_________________________________________________________________\n",
      "UpSampling0_1 (UpSampling3D) (None, 64, 64, 64, 64)    0         \n",
      "_________________________________________________________________\n",
      "upConvSam0_1 (Conv3D)        (None, 64, 64, 64, 32)    16416     \n",
      "_________________________________________________________________\n",
      "upConv0_1 (Conv3D)           (None, 64, 64, 64, 32)    27680     \n",
      "_________________________________________________________________\n",
      "upAct0_1 (Activation)        (None, 64, 64, 64, 32)    0         \n",
      "_________________________________________________________________\n",
      "upAct0_2 (Activation)        (None, 64, 64, 64, 32)    0         \n",
      "_________________________________________________________________\n",
      "shape_error_outputs (Conv3D) (None, 64, 64, 64, 6)     198       \n",
      "=================================================================\n",
      "Total params: 1,073,542\n",
      "Trainable params: 1,073,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Import Benchmarking Model\n",
    "model=dl_model_obj2.fcnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 350 samples, validate on 150 samples\n",
      "Epoch 1/3\n",
      "336/350 [===========================>..] - ETA: 0s - loss: 0.0044 - mean_absolute_error: 0.0070\n",
      "Epoch 00001: val_loss improved from inf to 0.00424, saving model to ../trained_models/cross_member_assembly/fcnn_bn/models/fcnn_0\n",
      "350/350 [==============================] - 22s 62ms/sample - loss: 0.0044 - mean_absolute_error: 0.0070 - val_loss: 0.0042 - val_mean_absolute_error: 0.0068\n",
      "Epoch 2/3\n",
      "336/350 [===========================>..] - ETA: 0s - loss: 0.0044 - mean_absolute_error: 0.0067\n",
      "Epoch 00002: val_loss improved from 0.00424 to 0.00422, saving model to ../trained_models/cross_member_assembly/fcnn_bn/models/fcnn_0\n",
      "350/350 [==============================] - 21s 61ms/sample - loss: 0.0044 - mean_absolute_error: 0.0067 - val_loss: 0.0042 - val_mean_absolute_error: 0.0061\n",
      "Epoch 3/3\n",
      "336/350 [===========================>..] - ETA: 0s - loss: 0.0043 - mean_absolute_error: 0.0065\n",
      "Epoch 00003: val_loss improved from 0.00422 to 0.00405, saving model to ../trained_models/cross_member_assembly/fcnn_bn/models/fcnn_0\n",
      "350/350 [==============================] - 22s 62ms/sample - loss: 0.0043 - mean_absolute_error: 0.0065 - val_loss: 0.0041 - val_mean_absolute_error: 0.0061\n"
     ]
    }
   ],
   "source": [
    "##Train Benchmarking Model\n",
    "from tensorflow.keras import backend as K\n",
    "weight_path=model_path+'/fcnn_0'\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(weight_path, verbose=1, save_best_only='val_loss',save_weights_only=True)\n",
    "history=model.fit(x=input_conv_data,y=y_out_list, validation_split=0.3, epochs=epocs, batch_size=batch_size,callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:42<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not convergent solutions:  0\n",
      "Splitting Contionous and Categorical KCCs\n",
      "Valid Split\n",
      "Importing output data for stage:  {'stage_id': 2, 'output_data_files_x': ['DX_crossmember_test1_3.csv'], 'output_data_files_y': ['DY_crossmember_test1_3.csv'], 'output_data_files_z': ['DZ_crossmember_test1_3.csv'], 'output_test_data_files_x': ['DX_crossmember_test1_3.csv'], 'output_test_data_files_y': ['DY_crossmember_test1_3.csv'], 'output_test_data_files_z': ['DZ_crossmember_test1_3.csv']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:42<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not convergent solutions:  0\n",
      "Importing output data for stage:  {'stage_id': 6, 'output_data_files_x': ['DX_crossmember_test1_7.csv'], 'output_data_files_y': ['DY_crossmember_test1_7.csv'], 'output_data_files_z': ['DZ_crossmember_test1_7.csv'], 'output_test_data_files_x': ['DX_crossmember_test1_7.csv'], 'output_test_data_files_y': ['DY_crossmember_test1_7.csv'], 'output_test_data_files_z': ['DZ_crossmember_test1_7.csv']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:42<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not convergent solutions:  0\n"
     ]
    }
   ],
   "source": [
    "# Import Test Dataset\n",
    "test_input_file_names_x=config.encode_decode_construct['input_test_data_files_x']\n",
    "test_input_file_names_y=config.encode_decode_construct['input_test_data_files_y']\n",
    "test_input_file_names_z=config.encode_decode_construct['input_test_data_files_z']\n",
    "test_input_dataset=[]\n",
    "test_input_dataset.append(get_data.data_import(test_input_file_names_x,data_folder))\n",
    "test_input_dataset.append(get_data.data_import(test_input_file_names_y,data_folder))\n",
    "test_input_dataset.append(get_data.data_import(test_input_file_names_z,data_folder))\n",
    "test_kcc_dataset=get_data.data_import(test_kcc_files,kcc_folder)\n",
    "test_input_conv_data, test_kcc_subset_dump,test_kpi_subset_dump=get_data.data_convert_voxel_mc(vrm_system,test_input_dataset,point_index,test_kcc_dataset)\n",
    "\n",
    "kcc_regression_test,kcc_classification_test=hy_util.split_kcc(test_kcc_subset_dump)\n",
    "Y_out_test_list=[]\n",
    "Y_out_test_list.append(kcc_regression_test)\n",
    "Y_out_test_list.append(kcc_classification_test)\n",
    "\n",
    "y_shape_error_test_list=[]\n",
    "\n",
    "for encode_decode_construct in encode_decode_multi_output_construct:\n",
    "\t#importing file names for model output\n",
    "\tprint(\"Importing output data for stage: \",encode_decode_construct)\n",
    "\n",
    "\n",
    "\ttest_output_file_names_x=encode_decode_construct['output_test_data_files_x']\n",
    "\ttest_output_file_names_y=encode_decode_construct['output_test_data_files_y']\n",
    "\ttest_output_file_names_z=encode_decode_construct['output_test_data_files_z']\n",
    "\n",
    "\ttest_output_dataset=[]\n",
    "\ttest_output_dataset.append(get_data.data_import(test_output_file_names_x,data_folder))\n",
    "\ttest_output_dataset.append(get_data.data_import(test_output_file_names_y,data_folder))\n",
    "\ttest_output_dataset.append(get_data.data_import(test_output_file_names_z,data_folder))\n",
    "\n",
    "\ttest_output_conv_data, test_kcc_subset_dump,test_kpi_subset_dump=get_data.data_convert_voxel_mc(vrm_system,test_output_dataset,point_index,test_kcc_dataset)\n",
    "\n",
    "\ty_shape_error_test_list.append(test_output_conv_data)\n",
    "\n",
    "shape_error_test=np.concatenate(y_shape_error_test_list, axis=4)\n",
    "\n",
    "Y_out_test_list.append(shape_error_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Weights for the best model\n",
    "model.load_weights(weight_path)\n",
    "y_pred_list=model.predict(test_input_conv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for Consistent Output\n",
    "len(y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786432, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:05<00:00, 85.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model Segmentation Validation Metrics are \n",
      "MAE              0.002010\n",
      "MSE              0.001285\n",
      "RMSE             0.032435\n",
      "R2            -394.871946\n",
      "R2_Adjusted     -0.171528\n",
      "dtype: float64\n",
      "(786432, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:06<00:00, 82.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model Segmentation Validation Metrics are \n",
      "MAE             0.010409\n",
      "MSE             0.007118\n",
      "RMSE            0.081886\n",
      "R2            -34.017012\n",
      "R2_Adjusted     0.034769\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Test Metrics based on model output\n",
    "from metrics_eval import MetricsEval\n",
    "metrics_eval=MetricsEval();\n",
    "\n",
    "if(option_num==1):\n",
    "    eval_metrics_reg,accuracy_metrics_df_reg=metrics_eval.metrics_eval_base(y_pred_list,Y_out_test_list[0],logs_path)\n",
    "    accuracy_metrics_df_reg.to_csv(logs_path+'/metrics_test_regression.csv')\n",
    "\n",
    "if(option_num==2):\n",
    "    eval_metrics_cla,accuracy_metrics_df_cla=metrics_eval.metrics_eval_classification(y_pred_list,Y_out_test_list[0],logs_path)\n",
    "    accuracy_metrics_df_cla.to_csv(logs_path+'/metrics_test_classification.csv')\n",
    "\n",
    "if(option_num==3):\n",
    "    eval_metrics_cop_list=[]\n",
    "    accuracy_metrics_df_cop_list=[]\n",
    "    t=0\n",
    "    index=0\n",
    "    for i in range(output_heads):\n",
    "        y_cop_pred=y_pred_list[:,:,:,:,t:t+3]\n",
    "        y_cop_test=Y_out_test_list[2][:,:,:,:,t:t+3]\n",
    "        y_cop_actual=y_cop_test\n",
    "        y_cop_pred_vector=np.reshape(y_cop_pred,(y_cop_pred.shape[0],-1))\n",
    "        y_cop_test_vector=np.reshape(y_cop_test,(y_cop_test.shape[0],-1))\n",
    "        y_cop_pred_vector=y_cop_pred_vector.T\n",
    "        y_cop_test_vector=y_cop_test_vector.T\n",
    "        print(y_cop_pred_vector.shape)\n",
    "        eval_metrics_cop,accuracy_metrics_df_cop=metrics_eval.metrics_eval_cop(y_cop_pred_vector,y_cop_test_vector,logs_path)\n",
    "        eval_metrics_cop_list.append(eval_metrics_cop)\n",
    "        accuracy_metrics_df_cop_list.append(accuracy_metrics_df_cop)\n",
    "        accuracy_metrics_df_cop.to_csv(logs_path+'/metrics_test_cop_'+str(index)+'.csv')\n",
    "        print(\"The Model Segmentation Validation Metrics are \")\n",
    "        print(accuracy_metrics_df_cop.mean())\n",
    "        accuracy_metrics_df_cop.mean().to_csv(logs_path+'/metrics_test_cop_summary_'+str(index)+'.csv')     \n",
    "        t=t+3\n",
    "        index=index+1  \n",
    "\n",
    "if(option_num==4):\n",
    "    eval_metrics_reg,accuracy_metrics_df_reg=metrics_eval.metrics_eval_base(y_pred_list[0],Y_out_test_list[0],logs_path)\n",
    "    eval_metrics_cla,accuracy_metrics_df_cla=metrics_eval.metrics_eval_classification(y_pred_list[1],Y_out_test_list[1],logs_path)\n",
    "    accuracy_metrics_df_reg.to_csv(logs_path+'/metrics_test_regression.csv')\n",
    "    accuracy_metrics_df_cla.to_csv(logs_path+'/metrics_test_classification.csv')\n",
    "\n",
    "\n",
    "if(option_num==5):\n",
    "    eval_metrics_reg,accuracy_metrics_df_reg=metrics_eval.metrics_eval_base(y_pred_list,Y_out_test_list[0],logs_path)\n",
    "    accuracy_metrics_df_reg.to_csv(logs_path+'/metrics_test_regression.csv')\n",
    "    eval_metrics_cop_list=[]\n",
    "    accuracy_metrics_df_cop_list=[]\n",
    "    t=0\n",
    "    index=0\n",
    "    for i in range(output_heads):\n",
    "        y_cop_pred=y_pred_list[:,:,:,:,t:t+3]\n",
    "        y_cop_test=Y_out_test_list[2][:,:,:,:,t:t+3]\n",
    "        y_cop_actual=y_cop_test\n",
    "        y_cop_pred_vector=np.reshape(y_cop_pred,(y_cop_pred.shape[0],-1))\n",
    "        y_cop_test_vector=np.reshape(y_cop_test,(y_cop_test.shape[0],-1))\n",
    "        y_cop_pred_vector=y_cop_pred_vector.T\n",
    "        y_cop_test_vector=y_cop_test_vector.T\n",
    "        print(y_cop_pred_vector.shape)\n",
    "        eval_metrics_cop,accuracy_metrics_df_cop=metrics_eval.metrics_eval_cop(y_cop_pred_vector,y_cop_test_vector,logs_path)\n",
    "        eval_metrics_cop_list.append(eval_metrics_cop)\n",
    "        accuracy_metrics_df_cop_list.append(accuracy_metrics_df_cop)\n",
    "        accuracy_metrics_df_cop.to_csv(logs_path+'/metrics_test_cop_'+str(index)+'.csv')\n",
    "        print(\"The Model Segmentation Validation Metrics are \")\n",
    "        print(accuracy_metrics_df_cop.mean())\n",
    "        accuracy_metrics_df_cop.mean().to_csv(logs_path+'/metrics_test_cop_summary_'+str(index)+'.csv')     \n",
    "        t=t+3\n",
    "        index=index+1 \n",
    "    \n",
    "\n",
    "if(option_num==6):\n",
    "    eval_metrics_cla,accuracy_metrics_df_cla=metrics_eval.metrics_eval_classification(y_pred_list,Y_out_test_list[0],logs_path)\n",
    "    accuracy_metrics_df_cla.to_csv(logs_path+'/metrics_test_classification.csv')\n",
    "    eval_metrics_cop_list=[]\n",
    "    accuracy_metrics_df_cop_list=[]\n",
    "    t=0\n",
    "    index=0\n",
    "    for i in range(output_heads):\n",
    "        y_cop_pred=y_pred_list[:,:,:,:,t:t+3]\n",
    "        y_cop_test=Y_out_test_list[2][:,:,:,:,t:t+3]\n",
    "        y_cop_actual=y_cop_test\n",
    "        y_cop_pred_vector=np.reshape(y_cop_pred,(y_cop_pred.shape[0],-1))\n",
    "        y_cop_test_vector=np.reshape(y_cop_test,(y_cop_test.shape[0],-1))\n",
    "        y_cop_pred_vector=y_cop_pred_vector.T\n",
    "        y_cop_test_vector=y_cop_test_vector.T\n",
    "        print(y_cop_pred_vector.shape)\n",
    "        eval_metrics_cop,accuracy_metrics_df_cop=metrics_eval.metrics_eval_cop(y_cop_pred_vector,y_cop_test_vector,logs_path)\n",
    "        eval_metrics_cop_list.append(eval_metrics_cop)\n",
    "        accuracy_metrics_df_cop_list.append(accuracy_metrics_df_cop)\n",
    "        accuracy_metrics_df_cop.to_csv(logs_path+'/metrics_test_cop_'+str(index)+'.csv')\n",
    "        print(\"The Model Segmentation Validation Metrics are \")\n",
    "        print(accuracy_metrics_df_cop.mean())\n",
    "        accuracy_metrics_df_cop.mean().to_csv(logs_path+'/metrics_test_cop_summary_'+str(index)+'.csv')     \n",
    "        t=t+3\n",
    "        index=index+1 \n",
    "\n",
    "if(option_num==7):\n",
    "    eval_metrics_reg,accuracy_metrics_df_reg=metrics_eval.metrics_eval_base(y_pred_list,Y_out_test_list[0],logs_path)\n",
    "    accuracy_metrics_df_reg.to_csv(logs_path+'/metrics_test_regression.csv')\n",
    "    eval_metrics_cla,accuracy_metrics_df_cla=metrics_eval.metrics_eval_classification(y_pred_list,Y_out_test_list[0],logs_path)\n",
    "    accuracy_metrics_df_cla.to_csv(logs_path+'/metrics_test_classification.csv')\n",
    "    eval_metrics_cop_list=[]\n",
    "    accuracy_metrics_df_cop_list=[]\n",
    "    t=0\n",
    "    index=0\n",
    "    for i in range(output_heads):\n",
    "        y_cop_pred=y_pred_list[:,:,:,:,t:t+3]\n",
    "        y_cop_test=Y_out_test_list[2][:,:,:,:,t:t+3]\n",
    "        y_cop_actual=y_cop_test\n",
    "        y_cop_pred_vector=np.reshape(y_cop_pred,(y_cop_pred.shape[0],-1))\n",
    "        y_cop_test_vector=np.reshape(y_cop_test,(y_cop_test.shape[0],-1))\n",
    "        y_cop_pred_vector=y_cop_pred_vector.T\n",
    "        y_cop_test_vector=y_cop_test_vector.T\n",
    "        print(y_cop_pred_vector.shape)\n",
    "        eval_metrics_cop,accuracy_metrics_df_cop=metrics_eval.metrics_eval_cop(y_cop_pred_vector,y_cop_test_vector,logs_path)\n",
    "        eval_metrics_cop_list.append(eval_metrics_cop)\n",
    "        accuracy_metrics_df_cop_list.append(accuracy_metrics_df_cop)\n",
    "        accuracy_metrics_df_cop.to_csv(logs_path+'/metrics_test_cop_'+str(index)+'.csv')\n",
    "        print(\"The Model Segmentation Validation Metrics are \")\n",
    "        print(accuracy_metrics_df_cop.mean())\n",
    "        accuracy_metrics_df_cop.mean().to_csv(logs_path+'/metrics_test_cop_summary_'+str(index)+'.csv')     \n",
    "        t=t+3\n",
    "        index=index+1 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                 | 2/500 [00:00<00:26, 18.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Nominal COP\n",
      "Saving Files for VRM Plotting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:24<00:00, 20.46it/s]\n",
      "  1%|▍                                                                                 | 3/500 [00:00<00:24, 20.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Files for VRM Plotting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:23<00:00, 20.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Plotting and saving additional files for VRM Plotting\n",
    "from point_cloud_construction import GetPointCloud\n",
    "import voxel_config as vc\n",
    "\n",
    "get_point_cloud=GetPointCloud()\n",
    "\n",
    "cop_file_name=vc.voxel_parameters['nominal_cop_filename']\n",
    "cop_file_path='../resources/nominal_cop_files/'+cop_file_name\n",
    "\n",
    "#Read cop from csv file\n",
    "print('Importing Nominal COP')\n",
    "nominal_cop=vrm_system.get_nominal_cop(cop_file_path)\n",
    "deploy_path=logs_path\n",
    "\n",
    "t=0\n",
    "index=0\n",
    "\n",
    "for i in range(output_heads):\n",
    "    y_cop_pred=y_pred_list[:,:,:,:,t:t+3]\n",
    "    y_cop_test=Y_out_test_list[2][:,:,:,:,t:t+3] #Change List ID based on output type\n",
    "    y_cop_actual=y_cop_test\n",
    "\n",
    "    #Saving For Matlab Plotting\n",
    "    dev_pred_matlab_plot_x=np.zeros((len(y_cop_pred),point_dim))\n",
    "    dev_pred_matlab_plot_y=np.zeros((len(y_cop_pred),point_dim))\n",
    "    dev_pred_matlab_plot_z=np.zeros((len(y_cop_pred),point_dim))\n",
    "\n",
    "    dev_actual_matlab_plot_x=np.zeros((len(y_cop_pred),point_dim))\n",
    "    dev_actual_matlab_plot_y=np.zeros((len(y_cop_pred),point_dim))\n",
    "    dev_actual_matlab_plot_z=np.zeros((len(y_cop_pred),point_dim))\n",
    "\n",
    "    # Saving for Matlab plotting\n",
    "    print(\"Saving Files for VRM Plotting...\")\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    for i in tqdm(range(len(y_cop_pred))):\n",
    "        actual_dev=get_point_cloud.getcopdev(y_cop_actual[i,:,:,:,:],point_index,nominal_cop)\n",
    "        pred_dev=get_point_cloud.getcopdev(y_cop_pred[i,:,:,:,:],point_index,nominal_cop)\n",
    "        dev_pred_matlab_plot_x[i,:]=pred_dev[:,0]\n",
    "        dev_pred_matlab_plot_y[i,:]=pred_dev[:,1]\n",
    "        dev_pred_matlab_plot_z[i,:]=pred_dev[:,2]\n",
    "        dev_actual_matlab_plot_x[i,:]=actual_dev[:,0]\n",
    "        dev_actual_matlab_plot_y[i,:]=actual_dev[:,1]\n",
    "        dev_actual_matlab_plot_z[i,:]=actual_dev[:,2]\n",
    "\n",
    "    np.savetxt((logs_path+'/DX_pred_'+str(index)+'.csv'),dev_pred_matlab_plot_x, delimiter=\",\")\n",
    "    np.savetxt((logs_path+'/DY_pred_'+str(index)+'.csv'),dev_pred_matlab_plot_y, delimiter=\",\")\n",
    "    np.savetxt((logs_path+'/DZ_pred_'+str(index)+'.csv'),dev_pred_matlab_plot_z, delimiter=\",\")\n",
    "    \n",
    "    np.savetxt((logs_path+'/DX_actual_'+str(index)+'.csv'),dev_actual_matlab_plot_x, delimiter=\",\")\n",
    "    np.savetxt((logs_path+'/DY_actual_'+str(index)+'.csv'),dev_actual_matlab_plot_y, delimiter=\",\")\n",
    "    np.savetxt((logs_path+'/DZ_actual_'+str(index)+'.csv'),dev_actual_matlab_plot_z, delimiter=\",\")\t\n",
    "    t=t+3\n",
    "    index=index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
