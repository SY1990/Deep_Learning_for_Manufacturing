{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection using Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # Nvidia Quadro GV100\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # Nvidia Quadro M2000\n",
    "\n",
    "#Adding Path to various Modules\n",
    "sys.path.append(\"../core\")\n",
    "sys.path.append(\"../visualization\")\n",
    "sys.path.append(\"../utilities\")\n",
    "sys.path.append(\"../datasets\")\n",
    "sys.path.append(\"../trained_models\")\n",
    "sys.path.append(\"../config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing Required Modules\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "#Importing Config files\n",
    "import assembly_config as config\n",
    "import model_config as cftrain\n",
    "import hybrid_utils as hy_util\n",
    "#Importing required modules from the package\n",
    "from measurement_system import HexagonWlsScanner\n",
    "from assembly_system import VRMSimulationModel\n",
    "from wls400a_system import GetInferenceData\n",
    "from data_import import GetTrainData\n",
    "from encode_decode_model import Encode_Decode_Model\n",
    "from training_viz import TrainViz\n",
    "from metrics_eval import MetricsEval\n",
    "from keras_lr_multiplier import LRMultiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing from Assembly Config File....\n"
     ]
    }
   ],
   "source": [
    "print('Parsing from Assembly Config File....')\n",
    "\n",
    "data_type=config.assembly_system['data_type']\n",
    "application=config.assembly_system['application']\n",
    "part_type=config.assembly_system['part_type']\n",
    "part_name=config.assembly_system['part_name']\n",
    "data_format=config.assembly_system['data_format']\n",
    "assembly_type=config.assembly_system['assembly_type']\n",
    "assembly_kccs=config.assembly_system['assembly_kccs']\n",
    "assembly_kpis=config.assembly_system['assembly_kpis']\n",
    "voxel_dim=config.assembly_system['voxel_dim']\n",
    "point_dim=config.assembly_system['point_dim']\n",
    "voxel_channels=config.assembly_system['voxel_channels']\n",
    "noise_type=config.assembly_system['noise_type']\n",
    "mapping_index=config.assembly_system['mapping_index']\n",
    "\n",
    "system_noise=config.assembly_system['system_noise']\n",
    "aritifical_noise=config.assembly_system['aritifical_noise']\n",
    "data_folder=config.assembly_system['data_folder']\n",
    "kcc_folder=config.assembly_system['kcc_folder']\n",
    "kcc_files=config.assembly_system['kcc_files']\n",
    "test_kcc_files=config.assembly_system['test_kcc_files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing from Training Config File\n"
     ]
    }
   ],
   "source": [
    "#added for hybrid model\n",
    "categorical_kccs=config.assembly_system['categorical_kccs']\n",
    "\n",
    "print('Parsing from Training Config File')\n",
    "\n",
    "model_type=cftrain.model_parameters['model_type']\n",
    "output_type=cftrain.model_parameters['output_type']\n",
    "batch_size=cftrain.model_parameters['batch_size']\n",
    "epocs=cftrain.model_parameters['epocs']\n",
    "split_ratio=cftrain.model_parameters['split_ratio']\n",
    "optimizer=cftrain.model_parameters['optimizer']\n",
    "loss_func=cftrain.model_parameters['loss_func']\n",
    "regularizer_coeff=cftrain.model_parameters['regularizer_coeff']\n",
    "activate_tensorboard=cftrain.model_parameters['activate_tensorboard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating file Structure....\n"
     ]
    }
   ],
   "source": [
    "print('Creating file Structure....')\n",
    "\n",
    "folder_name=part_type\n",
    "train_path='../trained_models/'+part_type\n",
    "pathlib.Path(train_path).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "train_path=train_path+'/model_selection'\n",
    "pathlib.Path(train_path).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "model_path=train_path+'/models'\n",
    "pathlib.Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logs_path=train_path+'/selection_logs'\n",
    "pathlib.Path(logs_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plots_path=train_path+'/plots'\n",
    "pathlib.Path(plots_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tuned_path=train_path+'/tuned_model'\n",
    "pathlib.Path(tuned_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the Assembly System and Measurement System....\n",
      "Valid Output Stages and heads\n",
      "KCC sub-listing:  0\n",
      "Process Parameter Dimension:  157\n"
     ]
    }
   ],
   "source": [
    "#Objects of Measurement System, Assembly System, Get Inference Data\n",
    "print('Initializing the Assembly System and Measurement System....')\n",
    "measurement_system=HexagonWlsScanner(data_type,application,system_noise,part_type,data_format)\n",
    "vrm_system=VRMSimulationModel(assembly_type,assembly_kccs,assembly_kpis,part_name,part_type,voxel_dim,voxel_channels,point_dim,aritifical_noise)\n",
    "get_data=GetTrainData()\n",
    "\n",
    "\n",
    "kcc_sublist=cftrain.encode_decode_params['kcc_sublist']\n",
    "output_heads=cftrain.encode_decode_params['output_heads']\n",
    "encode_decode_multi_output_construct=config.encode_decode_multi_output_construct\n",
    "\n",
    "if(output_heads==len(encode_decode_multi_output_construct)):\n",
    "\tprint(\"Valid Output Stages and heads\")\n",
    "else:\n",
    "\tprint(\"Inconsistent model setting\")\n",
    "\n",
    "print(\"KCC sub-listing: \",kcc_sublist)\n",
    "\n",
    "#Check for KCC sub-listing\n",
    "if(kcc_sublist!=0):\n",
    "\toutput_dimension=len(kcc_sublist)\n",
    "else:\n",
    "\toutput_dimension=assembly_kccs\n",
    "\n",
    "print(\"Process Parameter Dimension: \",output_dimension)\n",
    "\n",
    "input_size=(voxel_dim,voxel_dim,voxel_dim,voxel_channels)\n",
    "\n",
    "model_depth=cftrain.encode_decode_params['model_depth']\n",
    "inital_filter_dim=cftrain.encode_decode_params['inital_filter_dim']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all Process Parameter\n"
     ]
    }
   ],
   "source": [
    "#importing file names for model input\n",
    "input_file_names_x=config.encode_decode_construct['input_data_files_x']\n",
    "input_file_names_y=config.encode_decode_construct['input_data_files_y']\n",
    "input_file_names_z=config.encode_decode_construct['input_data_files_z']\n",
    "\n",
    "input_dataset=[]\n",
    "input_dataset.append(get_data.data_import(input_file_names_x,data_folder))\n",
    "input_dataset.append(get_data.data_import(input_file_names_y,data_folder))\n",
    "input_dataset.append(get_data.data_import(input_file_names_z,data_folder))\n",
    "\n",
    "kcc_dataset=get_data.data_import(kcc_files,kcc_folder)\n",
    "\n",
    "if(kcc_sublist!=0):\n",
    "\tprint(\"Sub-setting Process Parameters: \",kcc_sublist)\n",
    "\tkcc_dataset=kcc_dataset.iloc[:,kcc_sublist]\n",
    "\ttest_kcc_dataset=test_kcc_dataset[:,kcc_sublist]\n",
    "else:\n",
    "\tprint(\"Using all Process Parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:40<00:00, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not convergent solutions:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing to point cloud data\n",
    "point_index=get_data.load_mapping_index(mapping_index)\n",
    "input_conv_data, kcc_subset_dump,kpi_subset_dump=get_data.data_convert_voxel_mc(vrm_system,input_dataset,point_index,kcc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Contionous and Categorical KCCs\n",
      "Valid Split\n"
     ]
    }
   ],
   "source": [
    "kcc_regression,kcc_classification=hy_util.split_kcc(kcc_subset_dump)\n",
    "Y_out_list=[]\n",
    "Y_out_list.append(kcc_regression)\n",
    "Y_out_list.append(kcc_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing output data for stage:  {'stage_id': 2, 'output_data_files_x': ['DX_crossmember_test1_3.csv'], 'output_data_files_y': ['DY_crossmember_test1_3.csv'], 'output_data_files_z': ['DZ_crossmember_test1_3.csv'], 'output_test_data_files_x': ['DX_crossmember_test1_3.csv'], 'output_test_data_files_y': ['DY_crossmember_test1_3.csv'], 'output_test_data_files_z': ['DZ_crossmember_test1_3.csv']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:40<00:00, 12.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not convergent solutions:  0\n",
      "Importing output data for stage:  {'stage_id': 6, 'output_data_files_x': ['DX_crossmember_test1_7.csv'], 'output_data_files_y': ['DY_crossmember_test1_7.csv'], 'output_data_files_z': ['DZ_crossmember_test1_7.csv'], 'output_test_data_files_x': ['DX_crossmember_test1_7.csv'], 'output_test_data_files_y': ['DY_crossmember_test1_7.csv'], 'output_test_data_files_z': ['DZ_crossmember_test1_7.csv']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:40<00:00, 12.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not convergent solutions:  0\n"
     ]
    }
   ],
   "source": [
    "y_shape_error_list=[]\n",
    "\n",
    "for encode_decode_construct in encode_decode_multi_output_construct:\n",
    "\t#importing file names for model output\n",
    "\tprint(\"Importing output data for stage: \",encode_decode_construct)\n",
    "\n",
    "\toutput_file_names_x=encode_decode_construct['output_data_files_x']\n",
    "\toutput_file_names_y=encode_decode_construct['output_data_files_y']\n",
    "\toutput_file_names_z=encode_decode_construct['output_data_files_z']\n",
    "\n",
    "\toutput_dataset=[]\n",
    "\toutput_dataset.append(get_data.data_import(output_file_names_x,data_folder))\n",
    "\toutput_dataset.append(get_data.data_import(output_file_names_y,data_folder))\n",
    "\toutput_dataset.append(get_data.data_import(output_file_names_z,data_folder))\n",
    "\n",
    "\toutput_conv_data, kcc_subset_dump,kpi_subset_dump=get_data.data_convert_voxel_mc(vrm_system,output_dataset,point_index,kcc_dataset)\n",
    "\n",
    "\ty_shape_error_list.append(output_conv_data)\n",
    "\n",
    "shape_error=np.concatenate(y_shape_error_list, axis=4)\n",
    "Y_out_list.append(shape_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model using Keras Tuner\n",
    "\n",
    "#Get Base Model\n",
    "#Reloading to change cache\n",
    "import importlib\n",
    "from model_base_arch import *\n",
    "#importlib.reload(model_base_arch)\n",
    "\n",
    "base_model_obj4=BaseModelArch(output_dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare Model Hyperparameters to Tune\n",
    "import kerastuner as kt\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "#HyperModel Defination\n",
    "def model_builder(hp):\n",
    "    \n",
    "    base_model=base_model_obj4.base_model_func(hp,inital_filter_dim,model_depth,categorical_kccs,voxel_dim=64,deviation_channels=3,output_heads=2)\n",
    "    \n",
    "    return base_model\n",
    "    \n",
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "  def on_train_end(*args, **kwargs):\n",
    "    IPython.display.clear_output(wait = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D CNN model successfully compiled\n",
      "3D CNN model successfully compiled\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/2\n",
      "400/400 [==============================] - ETA: 2:45 - loss: 3.0347 - regression_outputs_loss: 0.8334 - classification_outputs_loss: 0.6799 - shape_error_outputs_loss: 0.0080 - regression_outputs_mean_absolute_error: 0.6580 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.022 - ETA: 1:34 - loss: 3.4639 - regression_outputs_loss: 1.1684 - classification_outputs_loss: 0.5117 - shape_error_outputs_loss: 0.1037 - regression_outputs_mean_absolute_error: 0.8137 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.141 - ETA: 1:09 - loss: 3.2903 - regression_outputs_loss: 1.0614 - classification_outputs_loss: 0.5485 - shape_error_outputs_loss: 0.0705 - regression_outputs_mean_absolute_error: 0.7678 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.097 - ETA: 56s - loss: 3.2343 - regression_outputs_loss: 1.0120 - classification_outputs_loss: 0.5780 - shape_error_outputs_loss: 0.0542 - regression_outputs_mean_absolute_error: 0.7449 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.075 - ETA: 48s - loss: 3.2003 - regression_outputs_loss: 0.9815 - classification_outputs_loss: 0.5965 - shape_error_outputs_loss: 0.0443 - regression_outputs_mean_absolute_error: 0.7318 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.06 - ETA: 42s - loss: 3.1674 - regression_outputs_loss: 0.9587 - classification_outputs_loss: 0.6061 - shape_error_outputs_loss: 0.0377 - regression_outputs_mean_absolute_error: 0.7222 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.05 - ETA: 37s - loss: 3.1589 - regression_outputs_loss: 0.9512 - classification_outputs_loss: 0.6118 - shape_error_outputs_loss: 0.0329 - regression_outputs_mean_absolute_error: 0.7195 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 33s - loss: 3.1423 - regression_outputs_loss: 0.9440 - classification_outputs_loss: 0.6124 - shape_error_outputs_loss: 0.0295 - regression_outputs_mean_absolute_error: 0.7183 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 30s - loss: 3.1330 - regression_outputs_loss: 0.9485 - classification_outputs_loss: 0.6045 - shape_error_outputs_loss: 0.0269 - regression_outputs_mean_absolute_error: 0.7234 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 27s - loss: 3.1060 - regression_outputs_loss: 0.9455 - classification_outputs_loss: 0.5952 - shape_error_outputs_loss: 0.0246 - regression_outputs_mean_absolute_error: 0.7234 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.03 - ETA: 25s - loss: 3.1004 - regression_outputs_loss: 0.9594 - classification_outputs_loss: 0.5794 - shape_error_outputs_loss: 0.0229 - regression_outputs_mean_absolute_error: 0.7320 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.03 - ETA: 22s - loss: 3.0594 - regression_outputs_loss: 0.9645 - classification_outputs_loss: 0.5545 - shape_error_outputs_loss: 0.0213 - regression_outputs_mean_absolute_error: 0.7359 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.03 - ETA: 20s - loss: 3.0277 - regression_outputs_loss: 0.9652 - classification_outputs_loss: 0.5386 - shape_error_outputs_loss: 0.0201 - regression_outputs_mean_absolute_error: 0.7388 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.03 - ETA: 18s - loss: 3.0223 - regression_outputs_loss: 0.9659 - classification_outputs_loss: 0.5358 - shape_error_outputs_loss: 0.0189 - regression_outputs_mean_absolute_error: 0.7413 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.03 - ETA: 16s - loss: 3.0006 - regression_outputs_loss: 0.9620 - classification_outputs_loss: 0.5293 - shape_error_outputs_loss: 0.0180 - regression_outputs_mean_absolute_error: 0.7408 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.02 - ETA: 14s - loss: 2.9706 - regression_outputs_loss: 0.9572 - classification_outputs_loss: 0.5195 - shape_error_outputs_loss: 0.0172 - regression_outputs_mean_absolute_error: 0.7397 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.02 - ETA: 12s - loss: 2.9504 - regression_outputs_loss: 0.9527 - classification_outputs_loss: 0.5143 - shape_error_outputs_loss: 0.0164 - regression_outputs_mean_absolute_error: 0.7382 - classification_outputs_categorical_accuracy: 0.0037 - shape_error_outputs_mean_absolute_error: 0.0272   - ETA: 11s - loss: 2.9327 - regression_outputs_loss: 0.9537 - classification_outputs_loss: 0.5047 - shape_error_outputs_loss: 0.0157 - regression_outputs_mean_absolute_error: 0.7396 - classification_outputs_categorical_accuracy: 0.0035 - shape_error_outputs_mean_absolute_error: 0.02 - ETA: 9s - loss: 2.9140 - regression_outputs_loss: 0.9561 - classification_outputs_loss: 0.4933 - shape_error_outputs_loss: 0.0151 - regression_outputs_mean_absolute_error: 0.7419 - classification_outputs_categorical_accuracy: 0.0033 - shape_error_outputs_mean_absolute_error: 0.0254 - ETA: 7s - loss: 2.8991 - regression_outputs_loss: 0.9562 - classification_outputs_loss: 0.4861 - shape_error_outputs_loss: 0.0146 - regression_outputs_mean_absolute_error: 0.7430 - classification_outputs_categorical_accuracy: 0.0031 - shape_error_outputs_mean_absolute_error: 0.024 - ETA: 6s - loss: 2.8756 - regression_outputs_loss: 0.9592 - classification_outputs_loss: 0.4715 - shape_error_outputs_loss: 0.0141 - regression_outputs_mean_absolute_error: 0.7450 - classification_outputs_categorical_accuracy: 0.0030 - shape_error_outputs_mean_absolute_error: 0.024 - ETA: 4s - loss: 2.8697 - regression_outputs_loss: 0.9610 - classification_outputs_loss: 0.4670 - shape_error_outputs_loss: 0.0136 - regression_outputs_mean_absolute_error: 0.7464 - classification_outputs_categorical_accuracy: 0.0028 - shape_error_outputs_mean_absolute_error: 0.023 - ETA: 3s - loss: 2.8700 - regression_outputs_loss: 0.9620 - classification_outputs_loss: 0.4664 - shape_error_outputs_loss: 0.0132 - regression_outputs_mean_absolute_error: 0.7478 - classification_outputs_categorical_accuracy: 0.0027 - shape_error_outputs_mean_absolute_error: 0.022 - ETA: 1s - loss: 2.8312 - regression_outputs_loss: 0.9582 - classification_outputs_loss: 0.4510 - shape_error_outputs_loss: 0.0128 - regression_outputs_mean_absolute_error: 0.7461 - classification_outputs_categorical_accuracy: 0.0026 - shape_error_outputs_mean_absolute_error: 0.022 - 43s 109ms/sample - loss: 2.8121 - regression_outputs_loss: 0.9557 - classification_outputs_loss: 0.4441 - shape_error_outputs_loss: 0.0124 - regression_outputs_mean_absolute_error: 0.7455 - classification_outputs_categorical_accuracy: 0.0025 - shape_error_outputs_mean_absolute_error: 0.0216 - val_loss: 2.2922 - val_regression_outputs_loss: 0.8831 - val_classification_outputs_loss: 0.2540 - val_shape_error_outputs_loss: 0.0038 - val_regression_outputs_mean_absolute_error: 0.7248 - val_classification_outputs_categorical_accuracy: 0.0000e+00 - val_shape_error_outputs_mean_absolute_error: 0.0085\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - ETA: 30s - loss: 2.2129 - regression_outputs_loss: 0.8858 - classification_outputs_loss: 0.2187 - shape_error_outputs_loss: 0.0040 - regression_outputs_mean_absolute_error: 0.7192 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.00 - ETA: 29s - loss: 2.1313 - regression_outputs_loss: 0.8790 - classification_outputs_loss: 0.1846 - shape_error_outputs_loss: 0.0042 - regression_outputs_mean_absolute_error: 0.7194 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.00 - ETA: 28s - loss: 2.2269 - regression_outputs_loss: 0.8902 - classification_outputs_loss: 0.2212 - shape_error_outputs_loss: 0.0040 - regression_outputs_mean_absolute_error: 0.7247 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.00 - ETA: 27s - loss: 2.1145 - regression_outputs_loss: 0.8945 - classification_outputs_loss: 0.1607 - shape_error_outputs_loss: 0.0042 - regression_outputs_mean_absolute_error: 0.7268 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.00 - ETA: 25s - loss: 2.0904 - regression_outputs_loss: 0.8949 - classification_outputs_loss: 0.1482 - shape_error_outputs_loss: 0.0043 - regression_outputs_mean_absolute_error: 0.7282 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.00 - ETA: 24s - loss: 2.2168 - regression_outputs_loss: 0.8991 - classification_outputs_loss: 0.2071 - shape_error_outputs_loss: 0.0044 - regression_outputs_mean_absolute_error: 0.7295 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.00 - ETA: 23s - loss: 2.2135 - regression_outputs_loss: 0.9002 - classification_outputs_loss: 0.2044 - shape_error_outputs_loss: 0.0043 - regression_outputs_mean_absolute_error: 0.7310 - classification_outputs_categorical_accuracy: 0.0089 - shape_error_outputs_mean_absolute_error: 0.0093   - ETA: 21s - loss: 2.1374 - regression_outputs_loss: 0.8979 - classification_outputs_loss: 0.1686 - shape_error_outputs_loss: 0.0043 - regression_outputs_mean_absolute_error: 0.7297 - classification_outputs_categorical_accuracy: 0.0078 - shape_error_outputs_mean_absolute_error: 0.00 - ETA: 20s - loss: 2.1175 - regression_outputs_loss: 0.9001 - classification_outputs_loss: 0.1566 - shape_error_outputs_loss: 0.0042 - regression_outputs_mean_absolute_error: 0.7305 - classification_outputs_categorical_accuracy: 0.0069 - shape_error_outputs_mean_absolute_error: 0.00 - ETA: 19s - loss: 2.1086 - regression_outputs_loss: 0.8972 - classification_outputs_loss: 0.1550 - shape_error_outputs_loss: 0.0041 - regression_outputs_mean_absolute_error: 0.7289 - classification_outputs_categorical_accuracy: 0.0063 - shape_error_outputs_mean_absolute_error: 0.00 - ETA: 18s - loss: 2.0651 - regression_outputs_loss: 0.8962 - classification_outputs_loss: 0.1343 - shape_error_outputs_loss: 0.0041 - regression_outputs_mean_absolute_error: 0.7281 - classification_outputs_categorical_accuracy: 0.0057 - shape_error_outputs_mean_absolute_error: 0.00 - ETA: 16s - loss: 2.0950 - regression_outputs_loss: 0.8956 - classification_outputs_loss: 0.1499 - shape_error_outputs_loss: 0.0041 - regression_outputs_mean_absolute_error: 0.7272 - classification_outputs_categorical_accuracy: 0.0052 - shape_error_outputs_mean_absolute_error: 0.00 - ETA: 15s - loss: 2.0307 - regression_outputs_loss: 0.8945 - classification_outputs_loss: 0.1189 - shape_error_outputs_loss: 0.0041 - regression_outputs_mean_absolute_error: 0.7273 - classification_outputs_categorical_accuracy: 0.0048 - shape_error_outputs_mean_absolute_error: 0.00 - ETA: 14s - loss: 2.0383 - regression_outputs_loss: 0.8955 - classification_outputs_loss: 0.1217 - shape_error_outputs_loss: 0.0041 - regression_outputs_mean_absolute_error: 0.7278 - classification_outputs_categorical_accuracy: 0.0045 - shape_error_outputs_mean_absolute_error: 0.01 - ETA: 12s - loss: 1.9620 - regression_outputs_loss: 0.8928 - classification_outputs_loss: 0.0862 - shape_error_outputs_loss: 0.0040 - regression_outputs_mean_absolute_error: 0.7263 - classification_outputs_categorical_accuracy: 0.0042 - shape_error_outputs_mean_absolute_error: 0.01 - ETA: 11s - loss: 1.9469 - regression_outputs_loss: 0.8926 - classification_outputs_loss: 0.0789 - shape_error_outputs_loss: 0.0040 - regression_outputs_mean_absolute_error: 0.7263 - classification_outputs_categorical_accuracy: 0.0039 - shape_error_outputs_mean_absolute_error: 0.01 - ETA: 10s - loss: 1.9895 - regression_outputs_loss: 0.8932 - classification_outputs_loss: 0.0996 - shape_error_outputs_loss: 0.0040 - regression_outputs_mean_absolute_error: 0.7268 - classification_outputs_categorical_accuracy: 0.0037 - shape_error_outputs_mean_absolute_error: 0.01 - ETA: 9s - loss: 1.8887 - regression_outputs_loss: 0.8946 - classification_outputs_loss: 0.0478 - shape_error_outputs_loss: 0.0040 - regression_outputs_mean_absolute_error: 0.7278 - classification_outputs_categorical_accuracy: 0.0069 - shape_error_outputs_mean_absolute_error: 0.0106 - ETA: 7s - loss: 1.8869 - regression_outputs_loss: 0.8931 - classification_outputs_loss: 0.0483 - shape_error_outputs_loss: 0.0040 - regression_outputs_mean_absolute_error: 0.7270 - classification_outputs_categorical_accuracy: 0.0066 - shape_error_outputs_mean_absolute_error: 0.010 - ETA: 6s - loss: 1.9379 - regression_outputs_loss: 0.8918 - classification_outputs_loss: 0.0752 - shape_error_outputs_loss: 0.0040 - regression_outputs_mean_absolute_error: 0.7263 - classification_outputs_categorical_accuracy: 0.0063 - shape_error_outputs_mean_absolute_error: 0.010 - ETA: 5s - loss: 1.8429 - regression_outputs_loss: 0.8912 - classification_outputs_loss: 0.0283 - shape_error_outputs_loss: 0.0039 - regression_outputs_mean_absolute_error: 0.7263 - classification_outputs_categorical_accuracy: 0.0060 - shape_error_outputs_mean_absolute_error: 0.010 - ETA: 3s - loss: 1.8091 - regression_outputs_loss: 0.8914 - classification_outputs_loss: 0.0112 - shape_error_outputs_loss: 0.0039 - regression_outputs_mean_absolute_error: 0.7262 - classification_outputs_categorical_accuracy: 0.0057 - shape_error_outputs_mean_absolute_error: 0.010 - ETA: 2s - loss: 1.8121 - regression_outputs_loss: 0.8929 - classification_outputs_loss: 0.0113 - shape_error_outputs_loss: 0.0039 - regression_outputs_mean_absolute_error: 0.7272 - classification_outputs_categorical_accuracy: 0.0054 - shape_error_outputs_mean_absolute_error: 0.010 - ETA: 1s - loss: 1.8607 - regression_outputs_loss: 0.8943 - classification_outputs_loss: 0.0341 - shape_error_outputs_loss: 0.0039 - regression_outputs_mean_absolute_error: 0.7279 - classification_outputs_categorical_accuracy: 0.0052 - shape_error_outputs_mean_absolute_error: 0.010 - 37s 93ms/sample - loss: 1.8655 - regression_outputs_loss: 0.8954 - classification_outputs_loss: 0.0354 - shape_error_outputs_loss: 0.0039 - regression_outputs_mean_absolute_error: 0.7286 - classification_outputs_categorical_accuracy: 0.0050 - shape_error_outputs_mean_absolute_error: 0.0106 - val_loss: 1.6846 - val_regression_outputs_loss: 0.9292 - val_classification_outputs_loss: 0.0072 - val_shape_error_outputs_loss: 0.0029 - val_regression_outputs_mean_absolute_error: 0.7522 - val_classification_outputs_categorical_accuracy: 0.0000e+00 - val_shape_error_outputs_mean_absolute_error: 0.0084\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 001c74f9134728946f6a4cdf090a4530</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 1.684567756652832</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Depth: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-filter_root: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/bracket: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/epochs: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/initial_epoch: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/round: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D CNN model successfully compiled\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384/400 [===========================>..] - ETA: 2:37 - loss: 3.2199 - regression_outputs_loss: 0.9158 - classification_outputs_loss: 0.6914 - shape_error_outputs_loss: 0.0054 - regression_outputs_mean_absolute_error: 0.7192 - classification_outputs_categorical_accuracy: 0.0625 - shape_error_outputs_mean_absolute_error: 0.014 - ETA: 1:28 - loss: 8509732915335135232.0000 - regression_outputs_loss: 82731016192.0000 - classification_outputs_loss: 0.6246 - shape_error_outputs_loss: 8509732915335135232.0000 - regression_outputs_mean_absolute_error: 170305.6094 - classification_outputs_categorical_accuracy: 0.0312 - shape_error_outputs_mean_absolute_error: 1504327296.000 - ETA: 1:04 - loss: 5673155276890090496.0000 - regression_outputs_loss: 55154012160.0000 - classification_outputs_loss: 0.4825 - shape_error_outputs_loss: 5673155093638152192.0000 - regression_outputs_mean_absolute_error: 113537.6328 - classification_outputs_categorical_accuracy: 0.0208 - shape_error_outputs_mean_absolute_error: 1002884864.000 - ETA: 52s - loss: 4254866457667567616.0000 - regression_outputs_loss: 41365508096.0000 - classification_outputs_loss: 0.5040 - shape_error_outputs_loss: 4254866457667567616.0000 - regression_outputs_mean_absolute_error: 85153.3984 - classification_outputs_categorical_accuracy: 0.0156 - shape_error_outputs_mean_absolute_error: 752163648.0000  - ETA: 44s - loss: 3403893166134053888.0000 - regression_outputs_loss: 33092407296.0000 - classification_outputs_loss: 0.5393 - shape_error_outputs_loss: 3403893111158472704.0000 - regression_outputs_mean_absolute_error: 68122.8594 - classification_outputs_categorical_accuracy: 0.0125 - shape_error_outputs_mean_absolute_error: 601730944.00 - ETA: 38s - loss: 2836577638445045248.0000 - regression_outputs_loss: 27577006080.0000 - classification_outputs_loss: 0.5629 - shape_error_outputs_loss: 2836577546819076096.0000 - regression_outputs_mean_absolute_error: 56769.1562 - classification_outputs_categorical_accuracy: 0.0104 - shape_error_outputs_mean_absolute_error: 501442432.00 - ETA: 34s - loss: 2431352261524324352.0000 - regression_outputs_loss: 23637432320.0000 - classification_outputs_loss: 0.5792 - shape_error_outputs_loss: 2431352261524324352.0000 - regression_outputs_mean_absolute_error: 48659.3750 - classification_outputs_categorical_accuracy: 0.0089 - shape_error_outputs_mean_absolute_error: 429807808.00 - ETA: 30s - loss: 2127433228833783808.0000 - regression_outputs_loss: 20682754048.0000 - classification_outputs_loss: 0.5906 - shape_error_outputs_loss: 2127433228833783808.0000 - regression_outputs_mean_absolute_error: 42577.0352 - classification_outputs_categorical_accuracy: 0.0078 - shape_error_outputs_mean_absolute_error: 376081824.00 - ETA: 27s - loss: 1891051758963363328.0000 - regression_outputs_loss: 18384670720.0000 - classification_outputs_loss: 0.5988 - shape_error_outputs_loss: 1891051697879384064.0000 - regression_outputs_mean_absolute_error: 37846.3281 - classification_outputs_categorical_accuracy: 0.0069 - shape_error_outputs_mean_absolute_error: 334294944.00 - ETA: 24s - loss: 1701946583067026944.0000 - regression_outputs_loss: 16546203648.0000 - classification_outputs_loss: 0.6055 - shape_error_outputs_loss: 1701946555579236352.0000 - regression_outputs_mean_absolute_error: 34061.7656 - classification_outputs_categorical_accuracy: 0.0063 - shape_error_outputs_mean_absolute_error: 300865472.00 - ETA: 22s - loss: 1547224166424570112.0000 - regression_outputs_loss: 15042002944.0000 - classification_outputs_loss: 0.6106 - shape_error_outputs_loss: 1547224103952318464.0000 - regression_outputs_mean_absolute_error: 30965.3047 - classification_outputs_categorical_accuracy: 0.0057 - shape_error_outputs_mean_absolute_error: 273514048.00 - ETA: 20s - loss: 1418288819222522624.0000 - regression_outputs_loss: 13788503040.0000 - classification_outputs_loss: 0.6143 - shape_error_outputs_loss: 1418288773409538048.0000 - regression_outputs_mean_absolute_error: 28384.9160 - classification_outputs_categorical_accuracy: 0.0052 - shape_error_outputs_mean_absolute_error: 250721216.00 - ETA: 18s - loss: 1309189679282328576.0000 - regression_outputs_loss: 12727848960.0000 - classification_outputs_loss: 0.6173 - shape_error_outputs_loss: 1309189732143464448.0000 - regression_outputs_mean_absolute_error: 26201.5137 - classification_outputs_categorical_accuracy: 0.0048 - shape_error_outputs_mean_absolute_error: 231434976.00 - ETA: 16s - loss: 1215676130762162176.0000 - regression_outputs_loss: 11818716160.0000 - classification_outputs_loss: 0.6195 - shape_error_outputs_loss: 1215676130762162176.0000 - regression_outputs_mean_absolute_error: 24330.0293 - classification_outputs_categorical_accuracy: 0.0045 - shape_error_outputs_mean_absolute_error: 214903904.00 - ETA: 14s - loss: 1134631055378018048.0000 - regression_outputs_loss: 11030802432.0000 - classification_outputs_loss: 0.6211 - shape_error_outputs_loss: 1134631059959316480.0000 - regression_outputs_mean_absolute_error: 22708.0723 - classification_outputs_categorical_accuracy: 0.0042 - shape_error_outputs_mean_absolute_error: 200576976.00 - ETA: 13s - loss: 1063716614416891904.0000 - regression_outputs_loss: 10341377024.0000 - classification_outputs_loss: 0.6223 - shape_error_outputs_loss: 1063716614416891904.0000 - regression_outputs_mean_absolute_error: 21288.8613 - classification_outputs_categorical_accuracy: 0.0039 - shape_error_outputs_mean_absolute_error: 188040912.00 - ETA: 11s - loss: 1001145048862957056.0000 - regression_outputs_loss: 9733060608.0000 - classification_outputs_loss: 0.6227 - shape_error_outputs_loss: 1001145056947601408.0000 - regression_outputs_mean_absolute_error: 20036.6152 - classification_outputs_categorical_accuracy: 0.0037 - shape_error_outputs_mean_absolute_error: 176979680.0000 - ETA: 9s - loss: 945525879481681664.0000 - regression_outputs_loss: 9192335360.0000 - classification_outputs_loss: 0.6238 - shape_error_outputs_loss: 945525848939692032.0000 - regression_outputs_mean_absolute_error: 18923.5078 - classification_outputs_categorical_accuracy: 0.0035 - shape_error_outputs_mean_absolute_error: 167147472.0000  - ETA: 8s - loss: 895761359508961664.0000 - regression_outputs_loss: 8708528128.0000 - classification_outputs_loss: 0.6241 - shape_error_outputs_loss: 895761334191259648.0000 - regression_outputs_mean_absolute_error: 17927.5703 - classification_outputs_categorical_accuracy: 0.0033 - shape_error_outputs_mean_absolute_error: 158350240.00 - ETA: 6s - loss: 850973291533513472.0000 - regression_outputs_loss: 8273101824.0000 - classification_outputs_loss: 0.6238 - shape_error_outputs_loss: 850973277789618176.0000 - regression_outputs_mean_absolute_error: 17031.2266 - classification_outputs_categorical_accuracy: 0.0031 - shape_error_outputs_mean_absolute_error: 150432736.00 - ETA: 5s - loss: 810450753841441408.0000 - regression_outputs_loss: 7879144448.0000 - classification_outputs_loss: 0.6240 - shape_error_outputs_loss: 810450776747933696.0000 - regression_outputs_mean_absolute_error: 16220.2500 - classification_outputs_categorical_accuracy: 0.0030 - shape_error_outputs_mean_absolute_error: 143269264.00 - ETA: 4s - loss: 773612083212285056.0000 - regression_outputs_loss: 7521001472.0000 - classification_outputs_loss: 0.6239 - shape_error_outputs_loss: 773612051976159232.0000 - regression_outputs_mean_absolute_error: 15482.9990 - classification_outputs_categorical_accuracy: 0.0028 - shape_error_outputs_mean_absolute_error: 136757024.00 - ETA: 2s - loss: 739976775246533504.0000 - regression_outputs_loss: 7194001408.0000 - classification_outputs_loss: 0.6235 - shape_error_outputs_loss: 739976754331910144.0000 - regression_outputs_mean_absolute_error: 14809.8564 - classification_outputs_categorical_accuracy: 0.0027 - shape_error_outputs_mean_absolute_error: 130811072.00 - ETA: 1s - loss: 709144409611261312.0000 - regression_outputs_loss: 6894251520.0000 - classification_outputs_loss: 0.6232 - shape_error_outputs_loss: 709144386704769024.0000 - regression_outputs_mean_absolute_error: 14192.8076 - classification_outputs_categorical_accuracy: 0.0026 - shape_error_outputs_mean_absolute_error: 125360608.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 39s 98ms/sample - loss: 680778633226810880.0000 - regression_outputs_loss: 6618481152.0000 - classification_outputs_loss: 0.6224 - shape_error_outputs_loss: 680778635975589888.0000 - regression_outputs_mean_absolute_error: 13625.1240 - classification_outputs_categorical_accuracy: 0.0025 - shape_error_outputs_mean_absolute_error: 120346184.0000 - val_loss: 2.9873 - val_regression_outputs_loss: 0.8878 - val_classification_outputs_loss: 0.6061 - val_shape_error_outputs_loss: 0.0066 - val_regression_outputs_mean_absolute_error: 0.6986 - val_classification_outputs_categorical_accuracy: 0.0000e+00 - val_shape_error_outputs_mean_absolute_error: 0.0481\n",
      "Epoch 2/2\n",
      "384/400 [===========================>..] - ETA: 26s - loss: 3.0482 - regression_outputs_loss: 0.9199 - classification_outputs_loss: 0.6008 - shape_error_outputs_loss: 0.0069 - regression_outputs_mean_absolute_error: 0.7282 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 25s - loss: 3.0217 - regression_outputs_loss: 0.9113 - classification_outputs_loss: 0.5955 - shape_error_outputs_loss: 0.0081 - regression_outputs_mean_absolute_error: 0.7213 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 24s - loss: 2.9737 - regression_outputs_loss: 0.8897 - classification_outputs_loss: 0.5933 - shape_error_outputs_loss: 0.0077 - regression_outputs_mean_absolute_error: 0.7083 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 23s - loss: 2.9613 - regression_outputs_loss: 0.8835 - classification_outputs_loss: 0.5935 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.7011 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 22s - loss: 2.9683 - regression_outputs_loss: 0.8860 - classification_outputs_loss: 0.5944 - shape_error_outputs_loss: 0.0074 - regression_outputs_mean_absolute_error: 0.7012 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 21s - loss: 2.9864 - regression_outputs_loss: 0.8932 - classification_outputs_loss: 0.5964 - shape_error_outputs_loss: 0.0074 - regression_outputs_mean_absolute_error: 0.7059 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 19s - loss: 2.9871 - regression_outputs_loss: 0.8958 - classification_outputs_loss: 0.5941 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.7076 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 18s - loss: 2.9948 - regression_outputs_loss: 0.9006 - classification_outputs_loss: 0.5931 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.7093 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 17s - loss: 2.9854 - regression_outputs_loss: 0.8977 - classification_outputs_loss: 0.5913 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.7065 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 16s - loss: 2.9778 - regression_outputs_loss: 0.8941 - classification_outputs_loss: 0.5911 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.7036 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.04 - ETA: 15s - loss: 2.9664 - regression_outputs_loss: 0.8901 - classification_outputs_loss: 0.5895 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.7007 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.05 - ETA: 14s - loss: 2.9651 - regression_outputs_loss: 0.8898 - classification_outputs_loss: 0.5891 - shape_error_outputs_loss: 0.0072 - regression_outputs_mean_absolute_error: 0.7011 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.05 - ETA: 13s - loss: 2.9625 - regression_outputs_loss: 0.8910 - classification_outputs_loss: 0.5866 - shape_error_outputs_loss: 0.0072 - regression_outputs_mean_absolute_error: 0.7013 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.05 - ETA: 12s - loss: 2.9565 - regression_outputs_loss: 0.8904 - classification_outputs_loss: 0.5842 - shape_error_outputs_loss: 0.0072 - regression_outputs_mean_absolute_error: 0.7008 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.05 - ETA: 11s - loss: 2.9515 - regression_outputs_loss: 0.8904 - classification_outputs_loss: 0.5817 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.7009 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.05 - ETA: 9s - loss: 2.9457 - regression_outputs_loss: 0.8888 - classification_outputs_loss: 0.5805 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.6996 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.0505 - ETA: 8s - loss: 2.9464 - regression_outputs_loss: 0.8900 - classification_outputs_loss: 0.5796 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.7001 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.050 - ETA: 7s - loss: 2.9348 - regression_outputs_loss: 0.8884 - classification_outputs_loss: 0.5754 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.6989 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.050 - ETA: 6s - loss: 2.9291 - regression_outputs_loss: 0.8868 - classification_outputs_loss: 0.5741 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.6980 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.050 - ETA: 5s - loss: 2.9257 - regression_outputs_loss: 0.8872 - classification_outputs_loss: 0.5721 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.6984 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.050 - ETA: 4s - loss: 2.9215 - regression_outputs_loss: 0.8861 - classification_outputs_loss: 0.5710 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.6974 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.050 - ETA: 3s - loss: 2.9198 - regression_outputs_loss: 0.8866 - classification_outputs_loss: 0.5696 - shape_error_outputs_loss: 0.0073 - regression_outputs_mean_absolute_error: 0.6977 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.051 - ETA: 2s - loss: 2.9143 - regression_outputs_loss: 0.8853 - classification_outputs_loss: 0.5681 - shape_error_outputs_loss: 0.0074 - regression_outputs_mean_absolute_error: 0.6968 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.051 - ETA: 1s - loss: 2.9139 - regression_outputs_loss: 0.8863 - classification_outputs_loss: 0.5669 - shape_error_outputs_loss: 0.0074 - regression_outputs_mean_absolute_error: 0.6976 - classification_outputs_categorical_accuracy: 0.0000e+00 - shape_error_outputs_mean_absolute_error: 0.0511"
     ]
    }
   ],
   "source": [
    "#Define Objective and direction to converge In\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "obj=kt.Objective(\"regression_outputs\", direction=\"min\")\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = 'val_loss', \n",
    "                     max_epochs = 10,\n",
    "                     factor = 3,\n",
    "                     directory = 'OSER_A',\n",
    "                     project_name = 'OSER_Arch_Opt')\n",
    "\n",
    "tuner.search(input_conv_data, Y_out_list, validation_split=0.2, batch_size=16, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
